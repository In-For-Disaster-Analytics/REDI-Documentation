{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Markdown cell for writing text to go along with code! Writing Markdown looks like:\n",
    "\n",
    "# I am a top-level header\n",
    "## I am a second level header\n",
    "### Third level here\n",
    "I am regular text with *italics* and **bold**\n",
    "\n",
    "`I am a bit of code`\n",
    "\n",
    "    I am a tab-indented\n",
    "    block\n",
    "    of\n",
    "    code  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to Python. We'll check where we are (our \"working directory\") with the print working directory Bash command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at what is in this directory (aka \"folder\"), we can use the Bash command to list directory contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing data from a CSV file into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with Python by importing a package that includes tools to make tabular data processing easier.\n",
    "\n",
    "`pandas` is a package that gives you a data structure called \"DataFrames\" to work with your tabular data, and many more useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a `pandas` DataFrame object with the contents of the \"surveys.csv\" file. This way we can use Python to work with the data from the file. The file itself remains unchanged.\n",
    "\n",
    "We do this by \"reading in\" the data with a `pandas` function, and \"assigning\" them to a variable. The variable name here is `surveys_df`, but it could be  nearly anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df = pd.read_csv('surveys.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what type of object our variable `surveys_df` is with a built-in Python function called `type`. This function takes one \"argument\" in parentheses – the object that we want it to return the type of. It is written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(surveys_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the first few rows of our new `surveys_df` DataFrame object, we can use a function that comes with the DataFrame-type object itself, called `head`. The syntax here is a little different than before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `head` function still uses the parentheses to take an argument. However, this function \"comes from\" the DataFrame object. \n",
    "\n",
    "So, the argument is not the DataFrame \"species_df\". Rather, it is the number of rows — '10' — that we want to view from within our `surveys_df` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also assign the output of the `head` function to a new variable, this time called `surveys_sub`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_sub = surveys_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the full DataFrame, write it's variable name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "surveys_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting columns using labels (column names)\n",
    "\n",
    "We use square brackets \"[ ]\", instead of parentheses, to select a subset of a Python object. This includes `pandas` DataFrames. \n",
    "\n",
    "Let's first see which column we want to select by first getting a list of the columns in `surveys_df`. We can do this by using the `columns` \"attribute\" of the DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know our column names, we can select all data from a column named `species_id`  in the `surveys_df` DataFrame by using the column name. \n",
    "\n",
    "There are two ways to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Use the column name in square bracket notation.\n",
    "# By the way, any text after a \"#\" symbol within a code cell is treated as a comment and is not run as code!\n",
    "\n",
    "surveys_df['species_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Use the column name as an attribute of the DataFrame, this gives the same output.\n",
    "\n",
    "# surveys_df.species_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign our selected column to a variable and also take a look at what type of object this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_species = surveys_df['species_id']\n",
    "type(surveys_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a Series, including an index from the original DataFrame and the values from the `species_id` column at that index.\n",
    "\n",
    "Take note that this index has numeric values and they start at '0'.\n",
    "\n",
    "\n",
    "We can also select multiple columns, what's different about the notation here? Can you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df[['species_id', 'year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you don't use the 'extra' square brackets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['species_id', 'year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting Data using Criteria\n",
    "\n",
    "Now we'll look at subsetting rows of data based on certain criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want a subset of the data for a particular year of interest. \n",
    "\n",
    "First, we want to find the years this datasets covers. We could list all the years in the `year` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a long Series to look through. We can instead feed this Series directly to the `unique` function that comes with `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_years = pd.unique(surveys_df.year)\n",
    "\n",
    "# This time using the `print` function.\n",
    "\n",
    "print(data_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or put it all into one line without creating a new variable!\n",
    "\n",
    "print(pd.unique(surveys_df.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to subset the data by our year of choice. This is  also done using square bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df[surveys_df.year == 1996]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Operators:\n",
    "\n",
    "- Equals: ==\n",
    "- Not equals: !=\n",
    "- Greater than, less than: > or <\n",
    "- Greater than or equal to: >=\n",
    "- Less than or equal to: <="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df[(surveys_df.year >= 1980) & (surveys_df.year <= 1985)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key points\n",
    "- Python uses 0-based indexing, in which the first element in a data structure – like a DataFrame or Series – has an index of '0'.\n",
    "- In a DataFrame, portions of data can be accessed, either by column or by row.\n",
    "- Pandas enables many more common data exploration steps such as data indexing, slicing, and conditional subsetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing data from a DataFrame into a CSV file\n",
    "\n",
    "We use the \"to_csv\" function of the DataFrame object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_sub.to_csv('first_surveys_row.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go back to our Bash command to list directory contents and see if our file has been written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Dataframes (Bonus)\n",
    "\n",
    "\n",
    "Next we'll combine DataFrames by using columns in each dataset that contain values in common (a common unique identifier). \n",
    "\n",
    "Combining DataFrames using a common \"field\" or column is called “joining”. The columns containing the common values are called “join key(s)”. **Note**: This process of joining tables is similar to what we do with tables in an SQL database.\n",
    "\n",
    "Joining DataFrames in this way is often useful when one DataFrame is a “lookup” containing additional data that we want to include in the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create another DataFrame for our \"species.csv\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_df = pd.read_csv('species.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CSV is a \"lookup\" of species information. We can take a closer look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many species does it list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It includes 54 species and the 4 columns we previously looked at. These species are identified in our survey data as well using the unique species code.\n",
    "\n",
    "Now we can glance at the first 5 rows with the `head` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the Surveys and Species data seperate makes sense for entering and storing data. Rather than adding 3 more columns for the genus, species and taxa to each of the 35,549 lines in the \"surveys.csv\", we can maintain the shorter table with the species information. \n",
    "\n",
    "**Note**: Storing data in this way has many benefits including:\n",
    "\n",
    "- It ensures consistency in the spelling of species attributes (genus, species and taxa) given each species is only entered once.\n",
    "- It also makes it easy for us to make changes to the species information once without having to find each instance of it in the larger survey data.\n",
    "- It optimizes the size of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to add this species data to our existing `surveys_df` for analysis?\n",
    "\n",
    "We can \"join\" the additional columns of information in Species to the Survey data. \n",
    "\n",
    "To make this easier to look at, we'll grab a subset of data to work with using the `head` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_sub = species_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a `species_sub` DataFrame with 20 rows in addition to the `surveys_sub` DataFrame we made earlier with 10 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to identify appropriate join keys we first need to know which field(s) are the same between the DataFrames. We can inspect both DataFrames to identify these columns. \n",
    "\n",
    "If we are lucky, both DataFrames will have columns with the same name that also contain the same data. If we are less lucky, we need to identify a (differently-named) column in each DataFrame that contains the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_sub.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the join key is the column containing the two-letter species identifier, which is called `species_id`.\n",
    "\n",
    "Now that we know the fields with the common species ID attributes in each DataFrame, we are almost ready to join our data. \n",
    "\n",
    "But wait! There are different types of joins. We also need to decide which type of join makes sense for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join\n",
    "\n",
    "The most common type of join is called an \"inner join\". An inner join combines two DataFrames based on a join key and returns a new DataFrame that contains **only** those rows that have matching values in **both** of the original DataFrames.\n",
    "\n",
    "Inner joins yield a DataFrame that contains only rows where the value being joined exists in both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inner = pd.merge(left=surveys_sub, right=species_sub, left_on='species_id', \n",
    "                        right_on='species_id')\n",
    "\n",
    "merged_inner.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `merged_inner` has fewer rows (7) than `surveys_sub` (10). This is an indication that there were rows in `surveys_sub` with value(s) for `species_id` that do not exist as value(s) for s`pecies_id` in `species_sub`.\n",
    "\n",
    "Let's take a look at our new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Join\n",
    "\n",
    "What if we want to add information from `species_sub` to `survey_sub` without losing any of the information from `survey_sub`? \n",
    "\n",
    "In this case, we use a different type of join called a “left outer join”, or a “left join”.\n",
    "\n",
    "Like an inner join, a left join uses join keys to combine two DataFrames. Unlike an inner join, a left join will return **all** of the rows from the left DataFrame.\n",
    "\n",
    "This includes rows whose join key(s) do not have values in the right DataFrame. Rows in the left DataFrame that are missing values for the join key(s) in the right DataFrame will simply have null (i.e., NaN or None) values for those columns.\n",
    "\n",
    "**Note**: a left join will still discard rows from the right DataFrame that do not have values for the join key(s) in the left DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we specify 'how'.\n",
    "\n",
    "merged_left = pd.merge(left=survey_sub,right=species_sub, how='left', \n",
    "                       left_on='species_id', right_on='species_id')\n",
    "\n",
    "merged_left.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we have the same number of rows as `survey_sub`. And we can take a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas `merge` function supports two other join types:\n",
    "\n",
    "- Right (outer) join: Invoked by passing `how='right'` as an argument. Similar to a left join, except all rows from the right DataFrame are kept, while rows from the left DataFrame without matching join key(s) values are discarded.\n",
    "\n",
    "- Full (outer) join: Invoked by passing `how='outer'` as an argument. This join type returns the all pairwise combinations of rows from both DataFrames; i.e., the result DataFrame will NaN where data is missing in one of the dataframes. This join type is very rarely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data from the joined DataFrame into a CSV file\n",
    "\n",
    "We use the \"to_csv\" function of the DataFrame object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inner.to_csv('top_surveys_species_merged_inner.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go back to our Bash command to list directory contents and see if our file has been written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
