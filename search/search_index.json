{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"REDI Solutions","text":"<p>REDI Soutions at TACC</p>"},{"location":"outline/","title":"REDI Documentation","text":"<p>Welcome to the home of documenation for the REDI Ecosystem.</p>"},{"location":"outline/#site-outline","title":"Site outline:","text":"<ul> <li>About  <ul> <li>Philosophy  </li> <li>Concept Models  </li> <li>Meet the Team  </li> </ul> </li> <li>REDI Ecosystem  <ul> <li>Integrated Solutions  <ul> <li>Dynamo  </li> <li>Dolce  </li> <li>Digital Twins  </li> </ul> </li> <li>Data Ecosystem  <ul> <li>Dynamo Model Catalog  </li> <li>DataX portal  </li> <li>Data  <ul> <li>Data Discovery  </li> <li>Data Registration  </li> </ul> </li> </ul> </li> <li>App Ecosystem  <ul> <li>NNA Sites &amp; Stories  </li> <li>Dynamo Cookbooks  </li> </ul> </li> <li>Infrastructure Resources  </li> </ul> </li> <li>Projects  <ul> <li>NNA Alaska  </li> <li>Subsidence  </li> <li>SETx KMO  </li> <li>RGV AI Enabled Modeling  </li> </ul> </li> <li>Field Protocols  <ul> <li>Drone  </li> <li>Geoslam (Geoslam)  </li> <li>Survey123  </li> </ul> </li> </ul>"},{"location":"about/","title":"Index","text":"<p>FAIR and CARE principles in practice.</p> <p>adhere to best practices of open science.</p> <p>[TODO: Get write up from Lydia]</p>"},{"location":"about/concept-models/","title":"Concept Models","text":""},{"location":"about/philosophy/","title":"Philosophy","text":""},{"location":"about/team/","title":"Meet the Team","text":""},{"location":"field-sessions/","title":"Field Session Write-ups","text":""},{"location":"field-sessions/#west-texas-fire-surveys","title":"West Texas: Fire Surveys","text":"<ul> <li>Fort Davis, March 2023<ul> <li>Aerial Drone Survey</li> <li>LiDAR (Geoslam)</li> </ul> </li> </ul>"},{"location":"field-sessions/fort-davis-march-23/drone/","title":"Unmanned Aerial Drone Operations","text":""},{"location":"field-sessions/fort-davis-march-23/drone/#fort-davis-mcdonald-observatory-field-research-trip","title":"Fort Davis-McDonald Observatory Field Research Trip","text":""},{"location":"field-sessions/fort-davis-march-23/drone/#federal-aviation-administration-faa-preparations","title":"Federal Aviation Administration (FAA) Preparations","text":"<p>The FAA defines an Unmanned Aircraft System (UAS) as an unmanned aircraft and all associated support equipment, control station, data links, telemetry, communications, and navigation equipment necessary to operate the unmanned aircraft. In simpler terms, it refers to any aircraft operated without a human pilot on board, controlled remotely or autonomously. This includes drones, quadcopters, and other similar unmanned aircraft.  </p> <p>Securing an FAA Remote Pilot Certificate is mandatory to operate your drone commercially within the United States under the Small UAS Rule (Part 107). This entails successfully passing a knowledge test, illustrating your comprehension of FAA regulations, airspace categorization, operational prerequisites, the weather's impact on drone flight, and other fundamental elements of safe drone operation. This certification stands as a vital measure to guarantee airspace safety and protect other aircraft, people, and property.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#general-steps-for-obtaining-a-remote-pilot-certificate","title":"General Steps for Obtaining a Remote Pilot Certificate","text":"<p>Self-Study. Test preparation courses for pay are available, but anyone with an aviation background or previous flight training can self-study using the reference, \u201cFAA Remote Pilot Study Guide\u201d located here: </p> <p>Other references recommended by the FAA, as revisions were made on April 21, 2021 to 14 CFR part 107 and a new 14 CFR part 89 for three new topics covered on the revised Unmanned Aircraft General (UAG) airman knowledge test: 1. Operations Over People 2. Remote Identification 3. Night Operations</p> <p>The new topics listed above will be added to the existing topic list: 4. Applicable regulations relating to small, unmanned aircraft system rating privileges, limitations, and flight operation 5. Effects of weather on small, unmanned aircraft performance 6. Small unmanned aircraft loading and performance 7. Emergency procedures 8. Crew resource management 9. Determining the performance of small, unmanned aircraft 10. Maintenance and preflight inspection procedures</p> <ul> <li>These changes will have a direct effect on airman knowledge testing for a Remote Pilot Certificate with a Small Aircraft Rating. Remote Pilot \u2013 Small Unmanned Aircraft Systems Airman Certification Standards (FAA-S-ACS-10B) </li> <li>Federal Register Vol. 86, No. 10, Operation of Small Unmanned Aircraft Systems Over People (Note: All new knowledge test questions were drafted from the rule only, beginning on page 68.) </li> <li>Federal Register Vol. 86, No. 10, Remote Identification of Unmanned Aircraft (Note: All new knowledge test questions were drafted from the rule only, beginning on page 116.) </li> <li>AC 107-2A, Small Unmanned Aircraft System (Small UAS)</li> <li>Pilot\u2019s Handbook of Aeronautical Knowledge (FAA-H-8083-25B) Chapter 17: Aeromedical Factors (Note: Applicable information found on pages 22 through 27 (pages 17-22 through 17-27 in full version of the handbook).) </li> <li>Get your FAA Tracking Number (FTN).You will need to have your FTN prior to scheduling and registering for any FAA Airman Knowledge Test scheduled on or after January 13, 2020.</li> </ul> <p>Find a Test Center. There are approximately 7,000 contracted testing centers throughout the United States. The testing supplement is called, \u201cAirman Knowledge Testing Supplement for Sport Pilot, Recreational Pilot, Remote Pilot, and Private Pilot.\u201d This supplement can help with practice tests.</p> <p>Apply for your Certificate. IACRA Application -- FAA's pilot management system. This is where you apply for your Part 107 Remote Pilot certificate (a nice checklist from an independent source). </p> <p>The exam fee is $175.00, the passing standard is 70 percent, and according to the FAA the pass rate was 85.68 percent in 2022, with an average score of 80.31 percent. The FAA will issue a temporary certificate in 10-15 business days and mail the permanent wallet license in 4-6 weeks. The temporary certificate must be in the pilot\u2019s possession while flying, until the permanent certificate arrives.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#faa-part-107-authorizations-and-businessprivate-flight-permissions","title":"FAA Part 107 Authorizations and Business/Private Flight Permissions","text":"<p>The FAA mandates that pilots adhere to Part 107 authorizations, which encompass essential checks like airspace assessments, weather evaluations. These requirements are crucial components of safe and responsible drone operation for commercial purposes. By conducting airspace checks, pilots ensure they are aware of any restrictions or regulations governing the area where they intend to fly. Similarly, performing weather checks allows pilots to assess conditions that could affect flight safety, such as high winds, precipitation, or low visibility, and other risk evaluations.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#drone-registration","title":"Drone Registration","text":"<p>Drone registration with the Federal Aviation Administration (FAA) is mandatory for all drones weighing between 0.55 pounds (250 grams) and 55 pounds (approximately 25 kilograms) that are operated outdoors in the United States for recreational or commercial purposes. This registration requirement is in place to help ensure accountability and safety in the national airspace system. Failure to register your drone can result in penalties and fines.</p> <p>To register your drone with the Federal Aviation Administration (FAA) in the United States, follow these steps:</p> <ul> <li>Create an Account: Visit the FAA's official drone registration website. If you do not already have an account, you will need to create one.</li> <li>Provide Information: Once logged in, you will be prompted to provide information about yourself and your drone. This typically includes your name, address, email, and details about your drone, such as its make, model, and serial number.</li> <li>Pay Registration Fee: There is a registration fee required by the FAA at $5.00 for each drone registration and is valid for three years. You will need to pay this fee using a credit or debit card.</li> <li>Receive Registration Number: After completing the registration process and payment, you will receive a unique registration number for your drone. This number must be affixed to your drone in a visible location.</li> <li>Keep Proof of Registration: It is essential to keep proof of registration with you whenever you are operating your drone. This can be a digital or printed copy of your registration certificate.</li> <li>Renew Registration: Remember to renew your drone registration every three years to maintain compliance with FAA regulations.</li> <li>Review Regulations: Familiarize yourself with the FAA's regulations for drone operation, including airspace restrictions, altitude limits, and any other requirements specific to your location or type of drone activity.</li> </ul>"},{"location":"field-sessions/fort-davis-march-23/drone/#airspace-preflight-check","title":"Airspace Preflight Check","text":"<p>The FAA also directs pilots to use LAANC (Low Altitude Authorization and Notification Capability) for obtaining authorization to fly drones in controlled airspace. LAANC is an automated system that enables drone operators to request and receive near real-time authorization to fly in airspace where air traffic control services are provided. By utilizing LAANC, pilots can efficiently obtain the necessary permissions to operate their drones safely within controlled airspace while ensuring compliance with FAA regulations. This streamlined process enhances airspace safety by providing operators with timely authorization and minimizing potential conflicts with crewed aircraft.</p> <p>Additionally, for government and public safety entities, FAA issues Certificates of Authorization (COA) allowing them to operate drones for specific purposes, such as law enforcement, search and rescue, or research.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#business-and-private-owner-flight-permissions","title":"Business and Private Owner Flight Permissions","text":"<p>Overall, communication and collaboration with property owners are key to ensuring safe and lawful drone flights over both business and private properties. By obtaining necessary permissions and adhering to regulations, drone pilots can conduct their operations responsibly while minimizing potential conflicts or concerns. This approach fosters a positive relationship between drone operators and the communities they serve while ensuring respect for property rights and privacy considerations.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#equipment","title":"Equipment","text":"<p>The primary drone-related equipment for the research field study included: - DJI Mavic 3 Multispectral drone:  The Mavic 3M is a professional-grade drone designed for agricultural applications and environmental monitoring. It combines the advanced flight capabilities of the DJI Mavic series with a multispectral imaging system, allowing users to capture detailed data for crop health analysis, vegetation mapping, and other specialized tasks. Our study collected orthophotos for change detection, and oblique photos to assist in damage assessments.</p> <p>Key features of the DJI Mavic 3 Multispectral include: Multispectral Camera: The drone is equipped with a specialized multispectral camera that captures images across multiple wavelengths of light, including visible and non-visible spectra such as near-infrared. This enables precise analysis of plant health, chlorophyll levels, and other vegetation characteristics. Multispectral Camera\u00a0</p> <ol> <li>Image Sensor\u00a0</li> <li>1/2.8-inch CMOS, effective pixels: 5 MP\u00a0</li> <li>Lens\u00a0</li> <li>FOV: 73.91\u00b0 (61.2\u00b0 x 48.10\u00b0)\u00a0</li> <li>Equivalent focal length: 25 mm\u00a0</li> <li>Aperture: f/2.0\u00a0</li> <li>Focus: Fixed Focus\u00a0</li> <li>Multispectral Camera Band\u00a0</li> <li>Green (G): 560 \u00b1 16 nm;\u00a0</li> <li>Red (R): 650 \u00b1 16 nm;\u00a0</li> <li>Red Edge (RE): 730 \u00b1 16 nm;\u00a0</li> <li>Near infrared (NIR): 860 \u00b1 26 nm;\u00a0</li> <li>Gain Range\u00a0</li> <li>1x-32x\u00a0</li> <li>Shutter Speed\u00a0</li> <li>Electronic Shutter: 1/30~1/12800 s\u00a0</li> <li>Max Image Size\u00a0</li> <li>2592\u00d71944\u00a0</li> <li>Image Format\u00a0</li> <li>TIFF\u00a0</li> <li>Video Format\u00a0</li> <li>MP4 (MPEG-4 AVC/H.264)\u00a0</li> <li>Photo Shooting Mode\u00a0<ol> <li>Single shot: 5 MP\u00a0</li> <li>Timelapse: 5 MP\u00a0</li> <li>TIFF: 2/3/5/7/10/15/20/30/60 s\u00a0</li> </ol> </li> <li>Video Resolution\u00a0<ol> <li>H.264\u00a0 </li> <li>FHD: 1920 x 1080@30fps\u00a0</li> <li>Video content: NDVI/GNDVI/NDRE\u00a0</li> </ol> </li> <li>Max Video Bitrate\u00a0<ol> <li>Stream: 60 Mbps</li> </ol> </li> </ol> <p>High-Resolution Imaging: The multispectral camera captures high-resolution imagery with detailed spectral information, allowing for accurate and comprehensive data analysis. Mapping Software with DJI Terra, or open source: The DJI Mavic 3 Multispectral is compatible with DJI Terra, a mapping and analysis software platform. Users can easily process and analyze the data captured by the drone to generate maps, 3D models, and actionable insights. Open-source software used in our project included OpenDroneMap ( https://opendronemap.org/ ), which provides solutions for collecting, processing, analyzing, and displaying aerial data. Flight Performance: Like other drones in the DJI Mavic series, the Mavic 3 Multispectral offers advanced flight performance and stability, with features such as obstacle avoidance, intelligent flight modes, and extended flight time. Compact and Portable Design: Despite its advanced capabilities, the Mavic 3 Multispectral maintains a compact and portable design, making it easy to transport and deploy in the field.</p> <ul> <li>RC Pro Enterprise Remote Controller. A remote controller, often referred to as a transmitter or transmitter controller, is used to control the drone's flight. It communicates wirelessly with the drone and allows the pilot to maneuver the aircraft, adjust its altitude, speed, and orientation, and control any onboard cameras or other features. Using the Pilot 2 software, mission planning was developed that enabled automated flight modes and detailed configurations.</li> </ul> <p>Smartphone: Prior to launch and where data services were available, a cell phone was used to check airspace and weather advisories, and recent or current weather observations.</p> <p>Batteries and Charger: Drones are powered by rechargeable batteries, so our team purchased three extra batteries (four total) and a charger. This was essential for extending flight time and ensuring uninterrupted operation during longer sessions. Each battery is rated for 43 min maximum flight time; however, windy conditions will reduce this time due to extra motor throttling to maintain flight attitude, altitude, and stability.</p> <p>Safety Equipment: Safety equipment such as a fire extinguisher, first aid kit, and high-visibility vest may be necessary, especially for commercial drone operations or flights in remote areas. We also brought sunglasses and sunscreen due to the sunny conditions.</p> <p>Accessories: We had various accessories like spare propellers, drone carrying case, USB cable, adapter and microSD card case and landing pad. Lens filters are an option, and can be important for convenience, protection, and enhancing the capabilities of the drone image and video capture.</p> <p>Personal Protective Equipment (PPE): Depending on the nature of the flight operations, personal protective equipment such as gloves, safety glasses, and a helmet may be necessary, particularly when flying in challenging or hazardous environments.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#flight-training","title":"Flight Training","text":"<p>The drone pilot for this research study had previously logged fixed wing and drone flight time but needed to become familiar with the DJI systems and configurations. Piloting any type of aircraft requires ongoing practice which can include both simulator training and real-world flying. Periodic check rides or practical flight tests by seasoned pilots are recommended, but not required for drones.</p> <p>Drone flight training is essential for aspiring pilots to develop the skills and knowledge necessary for safe and effective drone operation. Proper training not only enhances flight proficiency but also instills a strong understanding of regulations, airspace requirements, and emergency procedures.</p> <p>During drone flight training, pilots learn and hone their skills in fundamental flight maneuvers, navigation techniques, and best practices for various scenarios. They also become familiar with the features and capabilities of their specific drone model. Training programs often cover topics such as weather effects on flight, airspace classification, and privacy considerations.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#pre-flight-checklist","title":"Pre-Flight Checklist","text":"<p>One crucial aspect emphasized in drone flight training and day-to-day operations, is the importance of a pre-flight checklist. A pre-flight checklist is a systematic process that ensures all necessary steps are taken before each flight, reducing the risk of errors or oversights that could compromise safety.</p> <p>DJI includes a pre-flight checklist in its user manual (pg 31). This is the general checklist for most DJI drones:</p> <ol> <li>Pre-flight Inspection:<ol> <li>Remove gimbal protector before starting aircraft, otherwise damage to the camera system may occur.</li> <li>Check the drone for any physical damage, including the body, propellers, and landing gear.</li> <li>Ensure all components are securely attached.</li> <li>Ensure micro-SD card is inserted and the card slot cover is closed.</li> </ol> </li> <li>Battery Check:<ol> <li>Confirm that the battery is fully charged and properly inserted into the drone.</li> <li>Check the battery status indicators.</li> <li>Controller Check:</li> <li>Power on the remote controller and drone, and ensure it is paired with the drone.</li> <li>Check signal strength and Pilot 2 controller parameters, especially terrain avoidance and return to home (RTH) settings.</li> </ol> </li> <li>GPS and Compass Calibration:<ol> <li>Verify GPS connectivity and perform a compass calibration if necessary.</li> <li>Camera and Gimbal Check:</li> <li>Ensure the camera and gimbal are securely attached and functioning properly.</li> <li>Check camera settings and focus.</li> </ol> </li> <li>Flight Mode and Settings:<ol> <li>Select the appropriate flight mode for your mission.</li> <li>Load your flight plan if relevant.</li> <li>Review and adjust flight settings as needed.</li> <li>Obstacle Avoidance System:</li> <li>Verify that obstacle avoidance sensors are functioning correctly.</li> <li>Check sensor status indicators.</li> </ol> </li> <li>Propeller Check:<ol> <li>Inspect propellers for damage and ensure they are securely attached.</li> <li>Weather Conditions:</li> <li>Check weather forecasts and assess wind speed, precipitation, and visibility.</li> <li>Ensure conditions are suitable for safe flight.</li> <li>Emergency Procedures:</li> <li>Review emergency procedures, such as initiating return-to-home (RTH) or manual landing.</li> <li>Ensure familiarity with emergency features and actions that may need to be taken.</li> </ol> </li> </ol>"},{"location":"field-sessions/fort-davis-march-23/drone/#field-training-exercise-ftx","title":"Field Training Exercise (FTX)","text":"<p>03/04/2024, Long Canyon near Emma Long Metropolitan Park, Austin, Texas</p> <p>The objectives were to test all equipment prior to the Fort Davis field study. The drone pilot logged his first flights with the Mavic 3M and manually flew over a creek and residential home while taking video and multispectral images. The first stages of learning how to configure an automated flight route began, but there were a few critical settings and other technical roadblocks preventing the drone from taking off. These issues were troubleshot and solved in the first few days at Fort Davis.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#lessons-learned","title":"Lessons Learned","text":"<p>The Fort Davis Field Study from March 18-22, 2024, had many positive moments, most of which were preceded by numerous challenges. Some of the problems just needed to be solved, while other troubles could only be mitigated to some degree. Also, some of the lessons learned simply highlighted the need for additional training and familiarization, as the equipment was purchased only a month or so prior to the field study.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#rc-pro-controller-user-interface-user-experience-uiux","title":"RC Pro Controller User Interface / User Experience (UI/UX)","text":"<p>Next Button. The preflight check screen can be viewed whether the drone is on or off. When the drone is off, this screen is fixed with no scrolling. This was the norm, as the controller was studied in this state a lot before ever flying it. However, when the drone is running, the controller begins receiving data and populates numerous empty cells in the UI. Additionally, a new button appears on the screen, but it appears below the static screen UI and a thin scroll bar appears on the right margin. There is a button labeled, \u201cNext,\u201d which is only seen if you know to scroll down. This is what was initially missed at the earlier FTX which brought the automated flight plan to a halt. A very rudimentary step, but easily missed on day-1 with little familiarization. See figure 1.</p> <p> Figure 1. Screenshot of Preflight Check UI with drone off. No Next button.</p> <p>Max Altitude. The setting for Max Altitude in the Flight Controller Settings became an issue due to the higher elevation in West Texas (5,000 \u2013 6,800 ft MSL) when compared with Austin, Texas (780 ft MSL). This setting had to be increased to 500 ft for the drone to take off when flying an automated flight route. At the time, this was not fully understood, because the working setting was still below ground level compared with mean sea level (MSL) elevation. Later it was realized that the drone has an altimeter (aviation barometer), and at takeoff the drone altimeter height is reset to zero feet above ground level (AGL), or height relative to the surface. So, the earlier, lower max height values allowed manual flight at the lower flight levels, and once the system needed to cross check settings against the flight plan, the higher flight levels in the plan (ex. 390 ft) created a dependency and forced the higher max height setting. See Figure 2.</p> <p> Figure 2. Flight Controller Settings. Max Altitude and Max Flight Distance</p> <p>Geofencing. We wanted to create a custom geofence that would keep the drone out of specific geographic areas, however after much reading it was discovered that the Mavic 3M does not have a custom geofence capability. DJI Mavic 3 drones are equipped with built-in geofencing technology to enhance airspace safety and compliance with aviation regulations. DJI drones, including the Mavic 3, have predefined geofencing zones that include areas around airports, heliports, and other sensitive locations where drone flights are restricted or regulated. This also includes pre-programmed features such as, - No-Fly Zones - Restricted Zones - Dynamic Geofencing - Geofencing Alerts</p> <p>To mitigate this limitation, the pilot must use the Max Flight Distance setting shown in Figure 2. The drone will fly x distance and then stop and hover, until the pilot turns it around and flies it back toward the programmed home station. Not ideal, but it was used in some testing and worked. DJI allows the pilot to create polygons that keep the drone inside an area, but we were trying to create a polygon that would keep the drone outside an area. So, a better mitigation plan may be to create a flight plan where the area of interest polygon is digitized so a flight plan is built around the area of interest, like a square with a hole in the middle. The structure or land parcel to avoid is within the hole, and the rest of the polygon area is the collection zone. This would need to be tested, as it may try to fly through the hole for efficiency of flight paths, but that would violate the purpose of the defined planning area. Also, it may not recognize a complex polygon\u2019s geometry and fail to import the KML, or it may import the KML and ignore the hole.</p> <p>Another more nuanced method was devised to avoid areas when flying oblique imagery missions. In the first oblique flight it was discovered that the drone will fly five flight patterns. The first pattern collects NADIR-oriented orthophotos, then the remaining four passes collect oblique images, each at 90 degrees from the other. See figures 3.</p> <p> Figure 3. Orthophoto Pass #1. From flight plan on RC Pro Controller (Left)Oblique Passes #2 \u2013 5. From flight plan on RC Pro Controller(Right)</p> <p>Oblique photography needs to fly outside of the planning area by 40-50 percent of the length and width if you are flying a rectangular planning area. If a pilot is unaware of this behavior, the drone may fly into unauthorized areas adjacent to the collection area, so you must calculate for this in your collection planning. We called this area outside of the collection area \u201cspillage.\u201d On our second oblique flight, we estimated the spillage of the flight route, and the results were as expected. The drone dutifully stayed within the spillage zones, and within the main planning polygon area.</p> <p>Disconnected Maps in the RC Pro Controller. We had problems getting the RC Pro to recognize KMLs that we created in ArcGIS Pro. DJI allows users to create and load Keyhole Markup Language (KML) files and imagery into the Remote Controller (RC) Pro Controller for use in disconnected environments. Here is how this process works:</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#create-kmls-and-convert-imagery","title":"Create KMLs and Convert Imagery","text":"<p>Creation of KML Files: Users can create KML files using various mapping and GIS software programs, such as Google Earth, QGIS, or Esri ArcGIS Pro. These KML files contain geographic data, including waypoints, routes, and polygons, that define flight paths or areas of interest for drone missions.</p> <p>Converting KML Files into the Proper Format: Once the KML files are created, users need to check the axis order of the coordinate pairs for each vertex in the KML line or polygon. Within the EPSG database, the reference code 4326 corresponds to a geographic Coordinate Reference System (CRS) employing a (latitude, longitude) axis order. Yet, prevalent software in the industry interprets EPSG:4326 as a geographic CRS using a (longitude, latitude) axis order due to legacy OGC specifications being structured as such. This discrepancy extends beyond EPSG:4326, affecting numerous geographic CRS entries within the EPSG database. The axis order commonly recognized by existing software contradicts the specifications outlined in the EPSG database. A detailed historical account.</p> <p>One workaround is to open the KML file as a JSON file in a text reader like NotePad++. Then manually or programmatically swap the lon-lat pairs with lat-lon pairs.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#locate-and-crop-imagery-for-use-as-a-basemap","title":"Locate and Crop Imagery for Use as a Basemap","text":"<p>One method we experimented with was using an online resource called MyGeodata Converter. The service converts GeoTIFF images into MBTiles, which is the format required by the RC Pro. This worked well and is a free service for conversions that are &lt;5 MB. </p> <p>Loading KMLs and Imagery into RC Pro Controller: Users can connect their device running GS Pro to the Remote Controller (RC) Pro Controller via a compatible cable or wireless connection. Once connected, they can access the KML files and imagery stored in GS Pro and load them into the RC Pro Controller for use during flight missions.</p> <p>Mission Execution: With the KML files and imagery loaded into the RC Pro Controller, users can execute pre-planned drone missions, follow predefined flight paths, and capture imagery or data as specified in the mission plan.</p> <p>Data Collection and Analysis: After completing the drone mission, users can analyze the captured data and imagery for various applications, such as mapping, surveying, inspection, or environmental monitoring. The KMLs can also be reused in the analysis to maintain coordinate and geometry integrity, and accurate visualizations.</p> <p>MicroSD Card Compatibility: Our team discovered that our 256 GB Samsung PRO Plus V30 microSD cards did not work in the drone. This specific media was on the compatibility list specifically for the Mavic 3 Multispectral drone. We reformatted the cards to exFAT and were careful about our process and the order in which our steps occurred. The card was readable and writable on our Dell Toughbook, but the drone would not accept it. So, we used the included 64 GB Lexar U3 card for the drone and had no issues.</p> <p>When choosing a microSD card for the Mavic 3 Multispectral and RC Pro Controller, it is essential to consider the following factors:</p> <p>Write Speed: Look for microSD cards with fast write speeds to ensure smooth recording of high-resolution images and videos during flight. DJI often recommends cards with Class 10 or UHS-I Speed Class 3 (U3) ratings for optimal performance.</p> <p>Storage Capacity: Choose a microSD card with sufficient storage capacity to accommodate the volume of data expected to be generated during drone missions. DJI typically specifies maximum supported capacities to ensure compatibility.</p> <p>Compatibility: Ensure that the microSD card is compatible with the Mavic 3 Multispectral and RC Pro Controller. Recommended microSD cards: https://ag.dji.com/mavic-3-m/specs</p> <p>Remote Controller: - SanDisk Extreme PRO 64GB V30 A2 microSDXC - SanDisk High Endurance 64GB V30 microSDXC - SanDisk Extreme 128GB V30 A2 microSDXC - SanDisk Extreme 256GB V30 A2 microSDXC - SanDisk Extreme 512GB V30 A2 microSDXC - Lexar 667x 64GB V30 A2 microSDXC - Lexar High-Endurance 64GB V30 microSDXC - Lexar High-Endurance 128GB V30 microSDXC - Lexar 667x 256GB V30 A2 microSDXC - Lexar 512GB V30 A2 microSDXC - Samsung EVO Plus 64GB V30 microSDXC - Samsung EVO Plus 128GB V30 microSDXC - Samsung EVO Plus 256GB V30 microSDXC - Samsung EVO Plus 512GB V30 microSDXC - Kingston Canvas Go! Plus 128GB V30 A2 microSDXC - Kingston Canvas React Plus 128GB V90 A1 microSDXC Aircraft: - SanDisk Extreme 32GB V30 A1 microSDHC - SanDisk Extreme PRO 32GB V30 A1 microSDHC - SanDisk Extreme 512GB V30 A2 microSDXC - Lexar 1066x 64GB V30 A2 microSDXC (included) - Kingston Canvas Go! Plus 64GB V30 A2 microSDXC - Kingston Canvas React Plus 64GB V90 A1 microSDXC - Kingston Canvas Go! Plus 128GB V30 A2 microSDXC - Kingston Canvas React Plus 128GB V90 A1 microSDXC - Kingston Canvas React Plus 256GB V90 A2 microSDXC - Samsung PRO Plus 256GB V30 A2 microSDXC (purchased, but no-go for aircraft use)</p> <p>Reliability: Select microSD cards from reputable manufacturers known for producing high-quality, reliable storage media. Avoid generic or low-quality cards that may be prone to errors or failures.</p> <p>Drone Flight Speed During Imagery Collections. This was the final issue to be resolved before we had a 100 percent successful collection flight. At this point, we had flown 8-10 partial test missions, but had yet to acquire a valid collection of images. During a pivotal test collection flight at the Prude Ranch, the aircraft took off and began following a new orthoimagery collection plan and the pilot observed that the controller was showing that no pictures had been taken, yet the mission was 20 percent complete. We were expecting 800+ photos, so the flight was aborted.</p> <p>The pilot mentioned the flight had programmed a solution for 21 mph. One team member had the idea to try half that speed minus a small conservation factor bringing the new speed to 9 mph, so the next flight was programmed for 7 mph which gave us a finer pixel density of 5 cm at 395 ft flight altitude. That night, the pilot read about how flight speed affects image quality, and this variable is also impacted by drone flight elevation and the camera settings. After reviewing multiple resources, including a calculator running on Excel, a common range of speeds emerged which was 1-8 mph for orthophoto missions. These numbers were close to the fast manual estimation, so the next flight was programmed for 7 mph, a finer pixel density of 5 cm, and a 395 ft flight altitude. When the pilot started the new collection mission, the controller began making audible, faux camera shutter sounds. This was the first test mission that worked 100 percent and collected all expected images. It is important to remember that you can program. After inspecting the image quality that evening, there was some minor blurring, so we lowered the speed to 5 mph which showed a noticeable improvement. We also tested lower flight levels at 250 ft which gave us improved 3.5 cm pixel density.</p>"},{"location":"field-sessions/fort-davis-march-23/drone/#camera-settings","title":"Camera Settings","text":"<p>If the camera\u2019s shutter speed lags too much, there will also be image blurring. Our camera settings were set to Automatic, as we did not have any solid guidance at the time on potentially better modes like shutter priority. In shutter priority mode, after you set the desired shutter speed, the camera automatically adjusts the aperture to achieve the correct exposure based on the camera's metering system and the available light. The aperture opens wider for faster shutter speeds and narrows for slower shutter speeds.</p> <p>Exposure Compensation: In shutter priority mode, you can also use exposure compensation to adjust the overall exposure if needed. This allows you to make the image brighter or darker without changing the selected shutter speed. In future tests we should observe image quality differences and find one or more ideal configuration solutions by varying drone flight speed, flight elevation, enabling coordinated turns, and experimenting with shutter priority.</p>"},{"location":"field-sessions/fort-davis-march-23/lidar/","title":"Summary of hand-held LiDAR Acquisition during Fort Davis Field Exercise.","text":""},{"location":"field-sessions/fort-davis-march-23/lidar/#field-operations","title":"Field Operations","text":"<p>During the Fort Davis Field Exercise, hand-held LiDAR (Light Detection and Ranging) was acquired to evaluate the collection, in-field processing, and transfer of point cloud (3-dimentional) data to a centralized server for potential use during an emergency response scenario. The primary targets (located at Historic Prude Ranch and McDonald Observatory) were features meant to simulate damaged structures following a natural disaster event. During the exercise, the team located seven areas that contained sufficient structures and debris to simulate the damage and debris following a disaster event. The goal was to collect point-cloud data in the field, perform initial processing, and transfer the data to a server in a format that would be useful to emergency responders.</p> <p>The LiDAR equipment used for this exercise was the GeoSLAM ZEB Go Handheld Scanner.</p> <p></p> <p>The device consists primarily of a hand-held scanner and data logger which serves as temporary storage of the raw data files. Additional components included are a backpack, data cable, USB drives, 12V battery charger, and batteries. The ZEB Go unit is capable of scanning 43,000 scanner points per second, has a range of 30 meters, and relative accuracy of 1-3 cm.</p> <p>The hand-held scanner is comprised of two major components: a 2D time-of-flight laser range scanner coupled with an inertial measurement unit (IMU) on a scanning head which continuously rotates during operation of the scanner.</p> <p>Operation of the scanner is fairly simple. Once the data logger unit is powered on and the cable is connected between the logger and scanner, a simple boot-up procedure is performed with LED lights indicating each stage of the boot-up. With the data logger stowed in the backpack, the operator begins a systematic scan of the area of interest, moving the scanner up and down as they walk. The operator may walk in any direction, however, the acquisition must end in the approximate position it which it began (known as loop closure). Upon completion, the raw data is automatically compressed by the on-board software and is then manually transferred to a USB drive via the auxiliary port on the data logger. The proprietary zip files are given names based on date and time.</p> <p>The acquisitions varied in duration from 15-20 minutes each. The batteries (capable of 4 hours of continuous operation) were more than sufficient to cover 3-4 acquisitions in a day. The included battery charger was used nightly to recharge the batteries.</p> <p>Once transferred to a laptop, the FARO Connect software is used to convert the data to a 3D point cloud. During this process, the 2D laser data and the IMU data are merged using a 3D \u201csimultaneous localization and mapping\u201d (SLAM) algorithm. The software is capable of exporting to any number of common point-cloud formats. For this field exercise, the files were exported to LAS format which was subsequently imported into other software for further processing.</p> <p>More on the post-processing of the raw LiDAR data will be discussed in another section of this report.</p> <p>Links:</p> <p>GeoSLAM ZEB-Go Product Information Page:</p> <p>ZEB Go Handheld 3D Scanner: Laser Scanning for Everyone (geoslam.com)</p> <p>GeoSLAM ZEB-Go User Guide:</p> <p>https://geoslam.com/wp-content/uploads/2021/02/ZEB_Go_User_Guide_1.0.7.pdf</p> <p>FARO Connect Software:</p> <p>https://www.faro.com/en/Products/Software/FARO-Connect-Software</p>"},{"location":"field-sessions/fort-davis-march-23/lidar/#post-processing","title":"Post Processing","text":"<p>Faro Connect was the post processing. The Lidar collection starts in a proprietary .geoslam format that requires this software to download and process. The software goes through an algorithm to generate both a raw point cloud and a \u2018clean\u2019 version that removes anomalies and outliers from the cloud. We found the best results using the cleaned version of the point cloud format. Once exported it was imported into open-source 3d point cloud and mesh processing software CloudCompare.</p> <p>The point cloud data needed to be reprojected to a real world projection. To do so, an existing UTM Z13 point cloud from the 2019 USGS Lidar Campaign for the area was used. The results worked very well with areas that included easily defined structures for ground truthing. More work is necessary to rectify results in areas without such features.</p> <p>After reconciling the point cloud data, it could be imported into ArcGIS Pro and classified using its geoprocessing tools for LiDAR. After classification, filtering and surfaces can be created based on the desired use from DSMs to DTMs.</p>"},{"location":"integrated-solutions/","title":"Integrated Solutions","text":""},{"location":"integrated-solutions/#dolce-digital-object-life-cycle-ecosystem-data-management","title":"DOLCE: Digital Object Life Cycle Ecosystem  Data Management","text":"<p>Classification, Categorization &amp; Ingest of Digital Objects and Metadata at scale</p>"},{"location":"integrated-solutions/#digital-object-capture","title":"Digital Object Capture","text":"<ul> <li>Sites and Stories for unstructured data</li> <li>Spatial Surveying (link to something RE survey 123 stuff here)</li> <li>UPstream Sensor Database (upstream-dso.tacc.utexas.edu) for structured data</li> </ul>"},{"location":"integrated-solutions/#digital-object-registration","title":"Digital Object Registration","text":"<ul> <li>REDI Metadata Explorer Ingest and Register - This is the entry point for loading and registering data and data resources in the REDI solutions ecosystem.</li> <li>Metadata and Data layer ingest - Complete metadata model</li> <li>Embedded ingest for attachments 6G or less</li> <li>Model Catalog Service </li> </ul>"},{"location":"integrated-solutions/#digital-object-discovery","title":"Digital Object Discovery","text":"<ul> <li>REDI Data Discovery Services</li> </ul>"},{"location":"integrated-solutions/#dynamo-dynamic-analyses-and-modeling","title":"DYNAMO - Dynamic Analyses and Modeling","text":"<p>Model Integration as a Service! Dynamo is a dynamic modelling modelling platform that allows modelers to leverage HPC to facilitate running models</p> <ul> <li>PTDataX</li> <li>MIC and PTDataX Model Registry</li> <li>Cookbook Catalog</li> <li>MEEP </li> </ul>"},{"location":"integrated-solutions/#sociotechnical-digital-twins","title":"Sociotechnical Digital Twins","text":"<p>Digital Twins are poised to revolutionize simulations. The REDI team is developing tools that streamline the creation and orchestration of Digital Twins using physics-based information to support participatory processes</p>"},{"location":"integrated-solutions/#digital-twin-ready-data-and-services","title":"Digital Twin Ready Data and Services","text":"<ul> <li>Terrain Tile Service. DEM coverages processed from best available LIDAR data.  </li> <li>Spatial Visualization</li> <li>DT Transformation and Composition Platform. Tools and applications to support the creation and orchestration of digital twin analyses.    </li> </ul>"},{"location":"integrated-solutions/digital-twins/","title":"DIGITAL TWINS - Transformation and Orchestration","text":""},{"location":"integrated-solutions/dolce/","title":"Digital Object Life-Cycle Ecosystem (DOLCE)","text":"<p>The DAC Data Management Services team uses the Digital Object Life-Cycle Ecosystem (DOLCE) framework to assure robust ingest and metadata registry services as the base level requirements for core REDI services. Core capabilities currently include data ingest, assurance, management and storage. Future capabilities such as browsing, searching, exporting/downloading, and visualization are currently in development.</p> <p>The overall goal of DOLCE is to create policies and services that enable DAC to support accessibility to and discovery of digital objects throughout the phases of their lifecycle (see figure 2, above). These phases include:</p> <ul> <li>Generation - The creation of new data through direct observation, development of new models, or other primary research activities.</li> <li>Processing - Cleanup or other refining of data products including activities such as data transformation, removal of personally identifiable information, encryption, or compression.</li> <li>Description - Creation of robust metadata to facilitate long term discoverability, access, and reuse.</li> <li>Analysis - Use in scientific analysis to inform interpretation and gain meaningful insights from the digital objects. This includes using datasets for statistical analysis, as input for machine learning algorithms, to develop visualizations, and as input for modeling activities.</li> <li>Storage - Both short term storage for immediate access and long-term preservation.</li> <li>Sharing - Making the data available beyond the lifespan of its current project for reuse by other researchers in the short term or in the future.</li> </ul>"},{"location":"integrated-solutions/dolce/#guiding-principles","title":"Guiding Principles","text":"<p>DAC follows best practices generally accepted for free and open source software (FOSS), data management (FAIR Principles), and other standards. We encourage our users to familiarize themselves with these:</p> <ul> <li>FAIR (findable, accessible, interoperable, reusable) Principles</li> <li>Principles for Digital Development </li> <li>What is FOSS?</li> </ul>"},{"location":"integrated-solutions/dynamo/","title":"DYNAMO - DYnamic Analysis and MOdeling","text":""},{"location":"integrated-solutions/dynamo/#dynamo-dynamic-analysis-and-modeling","title":"DYNAMO: DYnamic Analysis and MOdeling","text":""},{"location":"integrated-solutions/dynamo/#ai-approaches-to-dynamic-analysis-modeling-and-information-management-for-groundwater-rivers-and-coasts","title":"AI approaches to dynamic analysis, modeling, and information management for groundwater, rivers, and coasts","text":""},{"location":"integrated-solutions/dynamo/#abstract","title":"Abstract","text":"<p>The DYNAMO allocation supports dynamic analysis and modeling workflows for physics-based systems with the goal of modernizing existing simulation tools and adding to the existing collection of models for the state of Texas. The DYNAMO compute allocations support testing and management of modeled information at scale for simulations data and tools. Modeling domains will include groundwater aquifers, fluvial and riverine surface water systems, and coastal systems. Additional domains of interest, such as subsidence and coastal spill conditions ,may be incorporated into DYNAMO applications and test cases.</p>"},{"location":"integrated-solutions/dynamo/#description","title":"Description","text":"<p>As part of the DYNAMO effort, storage and applications are in development to support big data collections composed of modeled outputs and ancillary or complementary datasets. The DYNAMO data collections need to follow FAIR principles for findable, accessible, interoperable, and reusable tools and collections.</p> <p>In addition to compute resources on Lonestar6 and Stampede3 on premise at TACC, the DYNAMO activities include the use of Microsoft Azure services for the AIM Flagship team of PT2050 . The Microsoft credits and funding have been pivotal for joint research applications with partners in the state of Texas. Over time we have developed a few approaches that leverage our on premise HPC systems with Azure. The resources complement each other and have helped us expand partnerships and research projects.</p> <p>The additional credits will help us continue supporting the Water Science Program at the Texas Water Development Board (TWDB). Project DYNAMO is a joint effort between PT2050 researchers and TWDB water science to scale physics-based modeling capabilities in Texas. The agency has been running flood simulation tests on the existing Azure credits resulting in a 20 - 50% speed up in model run times. Importantly, the services are highly valued because teams can share large models (&gt;120GB) for peer review more easily. Before using Azure teams were mailing hard drives across the state and between organizations. Azure has been key to the success of tests to date and with the latest gift credits we will continue providing access to our testbed Kubernetes on the cloud services to the agency.</p>"},{"location":"integrated-solutions/dynamo/#model-evaluation-environment","title":"Model Evaluation Environment","text":""},{"location":"integrated-solutions/dynamo/#access-instructions","title":"Access Instructions","text":"<p>The Decision Support Office at TACC created the Model Execution and Evaluation Platform (MEEP) as an environment that supports modeling tasks. The MEEP system provides a Windows Virtual Machines (VMs) with common flood modeling applications.  Follow these steps to access your MEEP Environment via Microsoft\u2019s Azure.</p>"},{"location":"integrated-solutions/dynamo/#1-create-a-microsoft-azure-account","title":"1. Create a Microsoft Azure account.","text":"<p>Navigate to the Azure Create Account page. Register with the email address you want to use for your MEEP user account. You may use your TACC user account and email address.</p>"},{"location":"integrated-solutions/dynamo/#2-log-in-to-azure","title":"2. Log in to Azure","text":"<p>Once you\u2019ve confirmed registration, notify the DSO team and TACC will set up your environment.  Go to: https://portal.azure.com/#@utexas.onmicrosoft.com/resource/subscriptions/25ddd913-58be-4c11-bde5-3ab19b3f6097/resourceGroups/hecras/providers/Microsoft.Compute/virtualMachines/Model-Evaluation-{LASTNAME}/overview </p> <p>Change *{LASTNAME}*  to your last name</p>"},{"location":"integrated-solutions/dynamo/#3-start-up-and-connect-to-the-virtual-machine","title":"3. Start up and Connect to the Virtual Machine","text":"<p>Once you\u2019ve logged into your VM, you\u2019ll see your landing page entitled  \u201cModel-Evaluation-YourLastName\u201d as demonstrated in Image 1. below. </p> <ul> <li>Click the Start button located below the Resource Name.</li> <li> <p>Scroll down on the left column until you see \u201cBastion\u201d. Click the \u201cBastion\u201d button.</p> <p></p> <p>Image 1. </p> </li> </ul>"},{"location":"integrated-solutions/dynamo/#4-continue-with-authorization","title":"4. Continue with Authorization","text":"<p>Fill in the following fields as shown in Image 2 below.  Do not edit other fields.</p> <ul> <li>Authorization Type to: Password from Azure Key Vault.</li> <li>Username: evaluator</li> <li>Subscription: TACC-MEEP</li> <li>Key vault: usacekeys</li> <li>Azure Key Vault Secret: evaluator</li> <li>**Click Connect. </li> </ul> <p>You should now have access to your VM!**</p> <p></p>"},{"location":"integrated-solutions/dynamo/#accessing-a-model","title":"Accessing a Model","text":"<p>Once you\u2019ve connected to your VM, you can use this machine like any Windows PC. The VM comes with Libre Office for word docs and excel files, and Microsoft Edge will allow you to read PDFs. You will find the files you need in the E: Drive. Please copy them over to the F: Drive while you are working on them.  Proceed with your model evaluation.</p> <p>Software is installed for the following models:</p> <ul> <li>HEC-RAS</li> <li>HEC-HMS</li> <li>HEC-SSP</li> <li>HEC-MetVue</li> </ul>"},{"location":"integrated-solutions/dynamo/#model-structure","title":"Model Structure","text":"<p>The Army Corps Evaluators will follow the SOP\u2019s to evaluate models.  Click to expand/collapse </p> <ul> <li>Region<ul> <li>Study Area<ul> <li>25Percent<ul> <li>Hydrologic Products<ul> <li>GIS</li> </ul> </li> <li>Hydrologic Features</li> <li>Hydrologic Losses<ul> <li>Land Cover</li> <li>Terrain</li> <li>Gauge Locations</li> </ul> </li> <li>Calculations and Supporting Data<ul> <li>Hydrologic Transform Calculations</li> <li>Hydrologic Loss Calculations</li> <li>Calibration and Validation</li> <li>Gage Data</li> <li>Statistical Hydrology</li> <li>Reservoir Data</li> <li>Elliptical Storm Data</li> <li>Final Hydrologic Results<ul> <li>Hydraulic Products</li> </ul> </li> <li>GIS</li> <li>HYDRAULICS</li> <li>FEATURES</li> <li>Final_files<ul> <li>Coastal Products</li> </ul> </li> <li>GIS</li> <li>HYDRAULICS</li> <li>FEATURES</li> <li>Final_files</li> </ul> </li> </ul> </li> <li>50Percent</li> <li>75Percent</li> <li>100Percent</li> </ul> </li> </ul> </li> </ul>"},{"location":"integrated-solutions/dynamo/#help","title":"Help","text":"<p>Contact DSO for your administrative needs and for technical issues with Azure. </p> <p>DSO POC: Tabish Khan tkhan@tacc.utexas.edu</p> <p>Secondary POCs: William Mobley wmobley@tacc.utexas.edu and Suzanne Pierce spiercey@tacc.utexas.edu</p>"},{"location":"integrated-solutions/dynamo/#issues","title":"Issues","text":"<ul> <li> <p>Getting a limited connection issue.</p> <p>Go back to the VM overview page. Confirm the Machine is \u201con\u201d if not press start. </p> </li> </ul> <p></p> <ul> <li> <p>Header Connection Error</p> <p>Go back to the VM overview page. Press the restart button try to relogin. </p> <p></p> </li> <li> <p>Connection error expired credentials</p> <ul> <li>Email wmobley@tacc.utexas.edu with a screenshot he will update the os to fix this issue.</li> </ul> </li> </ul> <p></p>"},{"location":"integrated-solutions/resources/developer-resources/","title":"Programmatic Resources","text":""},{"location":"integrated-solutions/resources/developer-resources/#data-ingest","title":"Data Ingest","text":"<ul> <li>Command Line Java app</li> </ul>"},{"location":"integrated-solutions/resources/developer-resources/#data-and-metadata-registration","title":"Data and Metadata Registration","text":"<ul> <li>REDI Core DB - is the centralized storage and registry location for information resources.</li> <li>Core DB Data Schema</li> </ul>"},{"location":"misc/","title":"Miscellaneous Folder","text":"<p>Don't know where your .md file should go? Dump it here!</p>"},{"location":"policies/","title":"Index","text":"<p>Section to include information about policies and governance</p>"},{"location":"projects/central-texas/","title":"Central Texas","text":""},{"location":"projects/central-texas/#description","title":"Description","text":"<p>The Smart &amp; Connected Communities project focuses on improving resilience in communities facing extreme weather events and chronic stressors. It aims to create a secure online portal where residents can share their local knowledge about these challenges. Researchers will work with the community to integrate this local knowledge with existing data sets used for planning and disaster preparation. The project not only enhances community resilience but also contributes to the national interest by providing a model that other communities can adapt. It involves creating innovative tools, data privacy processes, and knowledge integration frameworks, promoting local expertise in decision-making, and enhancing computer-based modeling for better resilience and preparedness.  </p> <p>Principle Funder: National Science Foundation Smart &amp; Connected Communities (Award No. 2127353)</p>"},{"location":"projects/nna-alaska/","title":"Alaskan Yukon","text":""},{"location":"projects/nna-alaska/#description","title":"Description","text":"<p>One of the Sites explored by this project is the Yukon Delta in Alaska through the Navigating the New Arctic (NNA) initiative. This project addresses key challenges for remote communities in Alaska which impact the nation's economy, security, and resilience. This project focuses on improving water infrastructure in rural Arctic areas by bridging the gap between engineering knowledge and local expertise. It employs an interdisciplinary approach in three phases to develop practical OMM resources grounded in local knowledge. The phases combine civil and environmental engineering, sociology, anthropology, and human-computer interaction to yield a multidisciplinary and multifaceted appraisal of the socio-environmental challenges in these communities.  </p> <p>Principle Funder: National Science Foundation Navigating the New Arctic Program (Award No. #1952196)</p>"},{"location":"projects/rgv/","title":"South Texas","text":""},{"location":"projects/rgv/#description","title":"Description:","text":"<p>The Sites and Stories project in the Rio Grande Valley of South Texas continues the community-engagement approach for the collection, organization, analysis, and correlation of human experiences with environmental issues. This methodology proves valuable in uncovering latent factors that may elude purely quantitative data analysis, especially in regions lacking robust infrastructure for data gathering. The Rio Grande Valley has a historical record marked by recurrent devastating flooding events, including those triggered by Hurricane Beulah, and as recently as the June 2018 flooding events which caused substantial destruction of infrastructure and property, and displaced many of the affected residents. However, segments of the community exist in unincorporated areas which are typically marginalized in decision-making, resource allocation, and disaster preparedness efforts. Consequently, these communities lack adequate quantitative data and requisite support. The incorporation of social science engagement adds a layer of sophistication, enabling the identification of underlying issues, root causes, and innovative solutions that cater to both the community and the environment.</p> <p>Principle Funder: Planet Texas 2050</p>"},{"location":"projects/setx/","title":"SETx","text":""},{"location":"projects/setx/#description","title":"Description:","text":"<p>Create the core infrastructure and templates for a reusable Knowledge Management Platform. Research and develop scalable decision support and deep uncertainty cybertools. How:\u00a0 Collaborate with research team to\u00a0design and implement services that support data and model management and extend CI and DSS approaches that will be sustained via leveraged efforts with other initiatives, like TDIS, DOLCE, MINT, etc. Research Focus (RF's) to develop advanced decision support tools that address: * RF1 - knowledge capture for sensed and unstructured information,\u00a0 * RF2 \u2013 develop reusable analytical methods that use natural language processing to computationally deconstruct problem formulations and link data/models with decision problems. * RF3 \u2013 explore and design methods and utilities that support a standardized use of methods for decision making under deep uncertainty. Implementation Science to support stakeholder community needs and act as a learning laboratory for Intelligent Systems and IFL researchers</p> <p>Principle Funder: Planet Texas 2050</p>"},{"location":"projects/subsidence/","title":"Subsidence","text":""},{"location":"protocols/","title":"Equipment and Software Protocols","text":""},{"location":"protocols/#unmanned-aerial-drone-operations","title":"Unmanned Aerial Drone Operations","text":"<p>Our research used unmanned aerial drone operations for surveying. A full writeup of our protocols and lessons learned is available for our field session in Fort Davis, Texas in March, 2023.  </p> <p>A more generalized set of protocols for this work will be published in this section soon.</p>"},{"location":"protocols/#hand-held-lidar-data-acquisition-using-geoslam","title":"Hand-held LiDAR data acquisition using Geoslam","text":"<p>Our research uses a hand-held GeoSLAM to collect ground level LiDAR (Light Detection and Ranging) data.  A full writeup of our protocols and lessons learned is available for our field session in Fort Davis, Texas in March, 2023.  </p> <p>A more generalized set of protocols for this work will be published in this section soon.</p>"},{"location":"protocols/#spatial-surveying-interview-data","title":"Spatial Surveying / Interview data","text":"<p>Spatial interview data is collected using custom survey templates produced with ArcGIS Survey123 software.  Visit the Spatial Surveys page for instructions on standing up a research survey on REDI systems.</p>"},{"location":"protocols/LiDAR_to_Shaded_Relief_Process/","title":"LiDAR to Shaded Relief Process","text":"<p>LiDAR to Shaded Relief \u2013 Process Steps</p> <ol> <li> <p>Prepare the workspace:</p> <ol> <li> <p>Create these folders: LAS, LAS_Datasets</p> </li> <li> <p>Create these geodatabases: DEMs.gdb, Merged_DEMs.gdb, Mosaic.gdb</p> </li> </ol> </li> <li> <p>Determine the necessary LiDAR las files</p> <ol> <li> <p>Download the TNRIS LiDAR Availability Index: https://tnris.org/data-download/#!/statewide (Tnris-lidar_tx.zip)</p> </li> <li> <p>Overlay the lidar index with the quarter quad index and other reference layers to determine the name of the acquisition (i.e. \u2013 CAPCOG 2007 140cm LiDAR), the las filenames, and the quarter quads that need to be downloaded.</p> </li> <li> <p>Important: Ensure the selected las grids create a complete rectangle. If you have an irregular set of grids, the resulting DEM below will be distorted</p> </li> </ol> </li> </ol> Correct Incorrect <ol> <li> <p>Download the LiDAR las files from TNRIS</p> <ol> <li> <p>Link: https://tnris.org/data-catalog/</p> </li> <li> <p>Click the lidar filter</p> </li> <li> <p>Click the A-Z button to put into alphabetic order (easier to locate the acquisition name)</p> </li> <li> <p>Click the acquisition you want</p> </li> <li> <p>Click the Launch Data Search and Download button.</p> </li> <li> <p>Type in the the name of the quad and select the correct one from the dropdown list. Since you already have the quad name, it is not necessary to enter a county.</p> </li> <li> <p>Scroll down to the LiDAR and Derived Products</p> </li> <li> <p>Download the correct quarter quad for the acquisiiton you are interested in.</p> </li> <li> <p>Save this to the LAS folder you created above.</p> </li> </ol> </li> <li> <p>Import the LAS files</p> <ol> <li> <p>Uncompress the LiDAR zip file</p> </li> <li> <p>If you haven\u2019t already done so, download and install the LASZip software.</p> </li> <li> <p>Go into the downloaded las subfolder and use LASZip to unzip the las files you need. You can either right click and Open With... or you can drag and drop the las file on top of the LASZip desktop icon.</p> </li> </ol> </li> <li> <p>Create LAS Dataset:</p> <ol> <li> <p>Open ArcPro and open the Create LAS Dataset tool</p> </li> <li> <p>Select the input LAS files</p> </li> <li> <p>Save the output to the LAS_Datasets folder</p> </li> <li> <p>For coordinate system, import from one of the las files to ensure its using the source\u2019s projection.</p> </li> <li> <p>Hit Run</p> </li> <li> <p>Review the new LAS dataset on the map to ensure it is falling into the right place. If it does not, the projection information is not defined. In these cases, use the Define Projection tool to create prj files for each las.</p> </li> </ol> </li> <li> <p>Create DEMs from LAS dataset</p> <ol> <li> <p>Open the LAS Dataset to Raster tool.</p> </li> <li> <p>Select the input LAS Dataset</p> </li> <li> <p>For Output, save the file to DEMs.gdb adding the suffix _DEM to the name.</p> </li> <li> <p>Set the sampling rate (For 140cm \u2013 1.2, For 50cm - .4, For 100cm - .9)</p> </li> <li> <p>Run</p> </li> </ol> </li> <li> <p>Create Hillshade</p> <ol> <li> <p>Open the Hillshade (Spatial Analyst) tool</p> </li> <li> <p>For Input, Select the DEM just created.</p> </li> <li> <p>For Output, save to DEMs.gbd adding the suffix _HS to the name.</p> </li> </ol> </li> <li> <p>Create an Mosaic Dataset (empty for now)</p> <ol> <li> <p>Open the Create Mosaic Dataset tool.</p> </li> <li> <p>For Output, save it to Mosaic.gdb, giving it the name of the area and a _Mosaic suffix.</p> </li> <li> <p>Import the projection from a source las file or your LAS Dataset.</p> </li> </ol> </li> <li> <p>Add Rasters to the Mosaic</p> <ol> <li> <p>Navigate to the new Mosaic in Catalog</p> </li> <li> <p>Right-click on it and select Add Rasters</p> </li> <li> <p>For Input Data, select Dataset</p> </li> <li> <p>Add the each of the DEMs created from the LAS Datasets (Remember, these should form a complete rectangle or the final image will be distorted)</p> </li> <li> <p>Under Raster Processing, check the boxes for Calulate Statistics and Build Raster Pyramids</p> </li> <li> <p>Under Mosaic Processing, check Build Thumnails, Update Overviews, and Estimate Mosaic Dataset Statistics</p> </li> <li> <p>Hit Run</p> </li> </ol> </li> <li> <p>Create Merged DEM from Mosaic</p> <ol> <li> <p>Open the new mosaic in the map</p> </li> <li> <p>Right-click and Export to Raster. Save this in Merged_DEMs.gdb</p> </li> </ol> </li> <li> <p>Create final Shaded Relief</p> <ol> <li> <p>Add new merged DEM to a map</p> </li> <li> <p>With the mosaic selected, go to the appearance tab. For Resampling Type, select cubic.</p> </li> <li> <p>Set symbology</p> <ol> <li> <p>Right-click the mosaic and open the symbology interface.</p> </li> <li> <p>For Color Scheme, select Bathymetric Scale (purple to red)</p> </li> <li> <p>For Stretch Type, select Standard Deviation</p> </li> </ol> </li> <li> <p>Add Hillshades</p> <ol> <li> <p>Add all of the hillshades to the map, placing them above the mosaic. Alternatively, these hillshades could be pulled into their own mosaic, but is not required when publishing a service.</p> </li> <li> <p>While on the Apppearance tab, select each hillshade and set the transparency to 48%.</p> </li> </ol> </li> </ol> </li> <li> <p>Publish service</p> <ol> <li> <p>Go to the Share tab</p> </li> <li> <p>Web Layer: Publish Web Layer</p> </li> <li> <p>For Name, include the name of the area with a suffix of _ShadedRelief</p> </li> <li> <p>Provide Summary and tags</p> </li> <li> <p>Leave Data and Layer Type at default settings (Map image)</p> </li> <li> <p>Set the output folder and share information</p> </li> <li> <p>Publish</p> </li> </ol> </li> </ol>"},{"location":"protocols/cookbooks/","title":"Cookbooks","text":"<p>The DSO team has lead an effort to share application across TACC infrastructure using the TAPIS systems. </p> <p>We've provided three templates to develop an application:  - Running a command - Running a Python script using conda - Running a Jupyter Notebook </p>"},{"location":"protocols/cookbooks/#requirements","title":"Requirements","text":"<ul> <li>A GitHub account</li> <li>TACC account. If you don't have one, you can request one here</li> <li>To access TACC systems, you should have an allocation</li> <li>You can see your allocations here</li> <li>If you don't have an allocation, you can request one here</li> </ul>"},{"location":"protocols/cookbooks/#template-overview","title":"Template Overview","text":"<p>This template creates a simple cookbook that will run a job on a TACC cluster using two parameters/arguments, Greeting and Target, and obtain the output via a UI, saving it to a file named out.txt.</p> <p></p> <p></p>"},{"location":"protocols/cookbooks/#how-does-it-work","title":"How does it work?","text":"<ol> <li><code>app.json</code> file: contains the definition of the Tapis application, including the application's name, description, Docker image, input files, and advanced options.</li> <li><code>Dockerfile</code>: a Docker image is built from the <code>Dockerfile</code>. The Docker image defines the runtime environment for the application and the files that will be used by the application.</li> <li><code>run.sh</code>: contains all the commands that will be executed on the TACC cluster.</li> </ol>"},{"location":"protocols/cookbooks/#job-run-script","title":"Job run script","text":"<p>The <code>run.sh</code> file is used to run the commands.</p> <pre><code>#!/bin/bash\n\nGreeting=$1\nTarget=$2\n\nFULL_GREETING=\"${Greeting} ${Target}. My name is ${_tapisJobOwner}\"\necho \"$FULL_GREETING\"\n\necho $FULL_GREETING &gt; $_tapisExecSystemOutputDir/out.txt\n</code></pre> <p>The <code>run.sh</code> script receives two parameters, <code>Greeting</code> and <code>Target</code>, and uses them to create a message that will be saved to a file named <code>out.txt</code>.</p> <p>Also, the script uses the <code>_tapisExecSystemOutputDir</code> variable, which contains the path where the application writes the output files.</p>"},{"location":"protocols/cookbooks/#create-your-cookbook","title":"Create your Cookbook","text":""},{"location":"protocols/cookbooks/#create-a-new-repository","title":"Create a new repository","text":"<ol> <li>Click on the \"Use this template\" button to create a new repository</li> <li>Fill in the form with the information for your new repository</li> </ol>"},{"location":"protocols/cookbooks/#build-the-docker-image","title":"Build the Docker image","text":"<p>You can either create your own docker image, use the one generated by Github, or use the Docker image from the Docker Hub.</p> <ol> <li>Clone the repository</li> <li>Build the Docker image using the command below</li> </ol> <pre><code>docker build -t cookbook-python .\n</code></pre> <ol> <li>Push the Docker image to a container registry</li> </ol> <pre><code>docker tag cookbook-python &lt;your-registry&gt;/cookbook-python\ndocker push &lt;your-registry&gt;/cookbook-python\n</code></pre> <p>The container image will be under the following address <pre><code>docker://ghcr.io/&lt;your-registry&gt;/&lt;repository&gt;:sha-&lt;commit number&gt;\n</code></pre></p>"},{"location":"protocols/cookbooks/#modify-the-appjson-file","title":"Modify the <code>app.json</code> file","text":"<p>Each app has a unique <code>id</code> and <code>description</code>. So, you should change these fields to match your app's name and description.</p> <ol> <li>Download the <code>app.json</code> file</li> <li>Change the values <code>id</code> and <code>description</code> fields with the name and description as you wish.</li> <li>If you built the Docker image, change the <code>containerImage</code> field with the image name you used.</li> </ol>"},{"location":"protocols/cookbooks/#create-a-new-application-on-the-cookbook-ui","title":"Create a New Application on the Cookbook UI","text":"<ol> <li>Go to Cookbook UI</li> <li>Click on the \"Create Application\" button</li> <li>Fill in the form with the information from your <code>app.json</code> file</li> <li>Click \"Create Application\"</li> <li>A new application will be created, and you will be redirected to the application's page</li> </ol>"},{"location":"protocols/cookbooks/#run-your-cookbook","title":"Run your Cookbook","text":"<ol> <li>Go to the application's page on the Cookbook UI, if you are not already there</li> <li>Click on the \"Run\" button on the right side of the page. This will open the Portal UI</li> <li>Select the parameters for your job    </li> </ol>"},{"location":"protocols/cookbooks/#check-the-output","title":"Check the Output","text":"<ol> <li>After the job finishes, you can check the output by clicking on the \"Output location\" link on the job's page    </li> <li>You will be redirected to the output location, where you can see the output files generated by the job    </li> <li>Click on a file to see its content. In this case, the file is named <code>out.txt</code> </li> </ol>"},{"location":"protocols/cookbooks/#next-templates","title":"Next templates","text":"<ul> <li>Running a command</li> <li>Running a Python script using conda</li> <li>Running a Jupyter Notebook</li> </ul>"},{"location":"protocols/cookbooks/#authors","title":"Authors","text":"<p>William Mobley - wmobley@tacc.utexas.edu Maximiliano Osorio</p>"},{"location":"protocols/drone/","title":"Unmanned Aerial Drone Operations","text":"<p>Our research used unmanned aerial drone operations for surveying. A full writeup of our protocols and lessons learned is available for our field session in Fort Davis, Texas in March, 2023.</p> <p>A more generalized set of protocols for this work will be published in this section soon.</p>"},{"location":"protocols/geoslam/","title":"Geoslam for LIDAR","text":""},{"location":"protocols/lidar/","title":"Hand-held LiDAR data acquisition using Geoslam","text":"<p>Our research uses a hand-held GeoSLAM to collect ground level LiDAR (Light Detection and Ranging) data.  A full writeup of our protocols and lessons learned is available for our field session in Fort Davis, Texas in March, 2023.</p> <p>A more generalized set of protocols for this work will be published in this section soon.</p>"},{"location":"protocols/mint/","title":"MINT","text":""},{"location":"protocols/mint/#model-registration-steps","title":"Model Registration Steps","text":""},{"location":"protocols/mint/#login","title":"Login","text":"<p>First, login to https://tacc.mint.isi.edu/ by using the username and password you received from your contact.</p>"},{"location":"protocols/mint/#configure-model","title":"Configure Model","text":"<p>Select Region -- Texas </p> <p>Go to Prepare Models and then Configure Models. Then Add new setup under MODFLOW (in Hydrology).</p> <p>For example,</p> <p></p> <p>The above model was setup for EDWARDS BFZ BARTON SPRINGS. For the setup, we need to add information like the description, Region, setup creator and input parameters. For the input parameters, all the GAM files like . ba6, . dis, . bc6, . dat and . hf6 files need to be registered. An example of registering a bas (.ba6) file can be seen below. The GAM file for Barton Springs was registered by adding external datasets to the 'Files' (as seen in the image below).</p> <p></p>"},{"location":"protocols/mint/#access-distribution-url-for-gaml-files","title":"Access Distribution URL for GAML files","text":"<p>Ingest Data into REDI services / Access Distribution URL for the GAM file: - CL method  --&gt; gives URL back - In-line using metadata registration tool --&gt; gives URL back - Method with survey attachments --&gt; gives URL back - Programmatically with REDI ingest service --&gt; give UID back. that can be used to  construct url from fragment + UID </p> <p>To add an external link/dataset, first, go to https://[ptdatax]{.underline}.tacc.utexas.edu/ . Next, go to Data Files - &gt;&gt; My Projects - &gt;&gt; Add New Project. Then upload the required GAMs and create a public URL. </p> <p>This public url can be added to the input parameter in the model setup. An example of public URL can be seen below --</p> <p></p> <p></p> <p>Example of a new setup under modflow - https://tacc.mint.isi.edu/texas/models/configure/MODFLOW/modflow_2005/modflow_2005_cfg/75e7e090-de07-42f4-a652-8202a8b1bd2e</p> <p>You can also go to \"see in catalog\" and visit the model catalog page -</p> <p></p> <p>This is how the model catalog page looks --</p> <p></p>"},{"location":"protocols/mint/#register-data-in-catalog","title":"Register data in catalog","text":"<p>The above steps showed how to create a model setup and hardcode external datasets to configure model. Next, we need to register the data in the catalog - https://data-catalog.tacc.mint.isi.edu/ using the GitHub code available at - https://github.com/mintproject/data_registration</p> <p>The code needs three files to run -- dataset.json, variables.json and resources.json. See example files : dataset_Barton.json, resources_Barton.json and variables_Barton.json files for more details.</p> <p>Once, you run the code, you get a record id and name of the model that was registered as seen below -</p> <p>Registering dataset</p> <p>{\\'result\\': \\'success\\', \\'datasets\\': [{\\'record_id\\': \\'ef0e8226-192b-4dee-a723-e445b492f4b4\\', \\'provenance_id\\': \\'9ef60317-5da5-4050-8bbc-7d6826fee49f\\', \\'name\\': \\'EDWARDS_BFZ-BARTON_SPRINGS-TRANSIENT-1989_1998_v1\\', \\'description\\': \\'Groundwater flow model developed for the Barton Springs segment of the Edwards Aquifer developed by the Bureau of Economic Geology, TWDB and Barton Springs Edwards Aquifer Conservation District. This region is hydrologically distinct from other parts of the Edwards Aquifer and is a major source of water.\\', \\'json_metadata\\': {\\'temporal_coverage\\': {\\'start_time\\': \\'1989-01-01T00:00:00\\', \\'end_time\\': \\'1998-01-01T00:00:00\\'}, \\'datatype\\': \\'modflow\\'}}]}</p> <p>Registering Variables</p> <p>{\\'result\\': \\'success\\', \\'standard_variables\\': [{\\'id\\': \\'65a6e85a-26d0-5a1f-bc37-3b0744f8adf4\\', \\'ontology\\': \\'SVO\\', \\'name\\': \\'aquifer_elevation\\', \\'uri\\': \\'http://www.geoscienceontology.org/svo/svl/variable#%28ground%40medium_water%29%40role%7Emain_water%40role%7aquifer_elevation\\', \\'description\\': \\'\\'}, {\\'id\\': \\'97f89b28-0dd7-5a0f-9b16-1c5b779bd93a\\', \\'ontology\\': \\'SVO\\', \\'name\\': \\'aquifer_specific_yield\\', \\'uri\\': \\'http://www.geoscienceontology.org/svo/svl/variable#%28ground%40medium_water%29%40role%7Emain_water%40role%7aquifer_specific_yield\\', \\'description\\': \\'\\'}, {\\'id\\': \\'9f017e2f-92e1-5fb9-9002-3aee59f3ec83\\', \\'ontology\\': \\'SVO\\', \\'name\\': \\'aquifer_horizontal_hydraulic_conductivity\\', \\'uri\\': \\'http://www.geoscienceontology.org/svo/svl/variable#%28ground%40medium_water%29%40role%7Emain_water%40role%7aquifer_horizontal_hydraulic_conductivity\\', \\'description\\': \\'\\'}, {\\'id\\': \\'f1ef8bb0-a32c-576e-822d-b3e337580118\\', \\'ontology\\': \\'SVO\\', \\'name\\': \\'aquifer_specific_storage\\', \\'uri\\': \\'http://www.geoscienceontology.org/svo/svl/variable#%28ground%40medium_water%29%40role%7Emain_water%40role%7Ein_recharge__recharge_volume_flux\\', \\'description\\': \\'\\'}]}</p> <p>{\\'result\\': \\'success\\', \\'variables\\': [{\\'record_id\\': \\'12e50d9d-55c1-40ca-bc73-3aa63f8b1bbc\\', \\'dataset_id\\': \\'ef0e8226-192b-4dee-a723-e445b492f4b4\\', \\'name\\': \\'elevation\\', \\'json_metadata\\': {\\'label\\': \\'elevation\\', \\'units\\': \\'ft\\', \\'data_type\\': \\'float\\', \\'type\\': \\'numerical.continuous\\'}}, {\\'record_id\\': \\'01fd9e97-23a6-4672-988e-cca609498ecc\\', \\'dataset_id\\': \\'ef0e8226-192b-4dee-a723-e445b492f4b4\\', \\'name\\': \\'Horizontal hydraulic conductivity\\', \\'json_metadata\\': {\\'label\\': \\'Horizontal hydraulic conductivity\\', \\'units\\': \\'ft/d\\', \\'data_type\\': \\'float\\', \\'type\\': \\'numerical.continuous\\'}}, {\\'record_id\\': \\'fff39f50-f4c0-4c02-93ef-22f3b867b409\\', \\'dataset_id\\': \\'ef0e8226-192b-4dee-a723-e445b492f4b4\\', \\'name\\': \\'specific yield\\', \\'json_metadata\\': {\\'label\\': \\'specific yield\\', \\'units\\': \\'\\', \\'data_type\\': \\'float\\', \\'type\\': \\'numerical.continuous\\'}}, {\\'record_id\\': \\'9ad79649-496b-4184-bba3-fb05272705be\\', \\'dataset_id\\': \\'ef0e8226-192b-4dee-a723-e445b492f4b4\\', \\'name\\': \\'specific storage\\', \\'json_metadata\\': {\\'label\\': \\'specific storage\\', \\'units\\': \\'ft-1\\', \\'data_type\\': \\'float\\', \\'type\\': \\'numerical.continuous\\'}}]}</p> <p>Registering Resources</p> <p>Registering resource chunk 1</p> <p>{\\'result\\': \\'success\\', \\'resources\\': [{\\'record_id\\': \\'c5fd7841-53a1-409f-8cd4-21933b11979a\\', \\'provenance_id\\': \\'9ef60317-5da5-4050-8bbc-7d6826fee49f\\', \\'dataset_id\\': \\'ef0e8226-192b-4dee-a723-e445b492f4b4\\', \\'name\\': \\'EDWARDS_BFZ-BARTON_SPRINGS-TRANSIENT-1989_1998\\', \\'resource_type\\': \\'model\\', \\'data_url\\': \\'https://tacc.mint.isi.edu/texas/models/configure/MODFLOW/modflow_2005/modflow_2005_cfg/75e7e090-de07-42f4-a652-8202a8b1bd2e\\', \\'layout\\': {}, \\'json_metadata\\': {\\'spatial_coverage\\': {\\'type\\': \\'BoundingBox\\', \\'value\\': {\\'xmax\\': -97, \\'xmin\\': -99, \\'ymax\\': 31, \\'ymin\\': 29}}, \\'temporal_coverage\\': {\\'start_time\\': \\'1989-01-01T00:00:00\\', \\'end_time\\': \\'1998-01-01T00:00:00\\'}}}, {\\'record_id\\': \\'09266e1a-e398-4345-a97b-abf9d7a88e06\\', \\'provenance_id\\': \\'9ef60317-5da5-4050-8bbc-7d6826fee49f\\', \\'dataset_id\\': \\'ef0e8226-192b-4dee-a723-e445b492f4b4\\', \\'name\\': \\'EDWARDS_BFZ-BARTON_SPRINGS-TRANSIENT-1989_1998.zip\\', \\'resource_type\\': \\'Input data zip file\\', \\'data_url\\': \\'https://portals-api.tacc.utexas.edu/postits/v2/38e48b08-b6bc-44e9-b105-f8063289d890-010\\', \\'layout\\': {}, \\'json_metadata\\': {\\'spatial_coverage\\': {\\'type\\': \\'BoundingBox\\', \\'value\\': {\\'xmax\\': -97, \\'xmin\\': -99, \\'ymax\\': 31, \\'ymin\\': 29}}, \\'temporal_coverage\\': {\\'start_time\\': \\'1989-01-01T00:00:00\\', \\'end_time\\': \\'1998-01-01T00:00:00\\'}}}]}</p> <p>After registering the data, the dataset name \\'EDWARDS_BFZ-BARTON_SPRINGS-TRANSIENT-1989_1998_v1\\' can be searched on https://data-catalog.tacc.mint.isi.edu/ as seen below --</p> <p></p> <p>The above figure shows both the temporal and spatial information of the dataset. If you click \"View more details\" you can see detailed information about the variables and resources.</p> <p></p> <p></p> <p>The resources include the link to the model setup and all the GAM files.</p> <p>Next, you can go to Explore Areas -&gt;&gt; Hydrology . If you correctly added the bounding box information in the json file, the hydrology map will show your model.</p> <p></p> <p>Ideally, when you click on the location in the hydrology map, it should show your dataset/model. But currently, the above map shows only 11 datasets for each hydrology region so if we keep adding more datasets to any region then you cannot see your data. Since Edwards_9 and Edwards_24 already have 11 models each, I used a different coordinate away from Edwards BFZ Barton Springs for a region which had less than 11 dataset registered in it.</p> <p></p> <p>You can also access the model on the hydrology map using the following link (need to add your record id and then the region id after https://tacc.mint.isi.edu/texas/datasets/browse) --</p> <p>https://tacc.mint.isi.edu/texas/datasets/browse/ef0e8226-192b-4dee-a723-e445b492f4b4/1ap0Inly6Lq3jYZgDDKy</p> <p></p> <p>User Comments --</p> <ol> <li>The search Dataset in https://data-catalog.tacc.mint.isi.edu/ resets itself to Ethiopia after every search. So, the user needs to refresh the page after every search -</li> </ol> <p></p> <ol> <li>Only 11 models are visible under each hydrology region in the hydrology map. So, your model may not show up in the hydrology map even after adding the correct spatial location. This needs to be fixed.</li> </ol> <p></p> <p>For example, I registered the Edwards Barton Spring model, which showed up on the hydrology map but disappeared later. This is because the hydrology region where the Barton Spring model was registered, already had 11 models. So, I changed the spatial coverage of this model to make it visible on the hydrology map as seen above (the model is now visible in the Seymour region). If you want to see the model in the correct location, you need to go to - https://tacc.mint.isi.edu/texas/datasets/browse/ef0e8226-192b-4dee-a723-e445b492f4b4/1ap0Inly6Lq3jYZgDDKy</p> <p>Additionally, it is hard to click on smaller regions on the hydrology map.</p> <ol> <li> <p>A user can modify an existing registered model by adding the record id in the dataset.json file but it is hard to overwrite or delete an existing model/dataset. The code keeps appending new information to the existing model but is not able to replace the metadata.</p> </li> <li> <p>It may be better to use a scripted approach to create the model setup instead of using GUI</p> </li> <li> <p>It is hard to create a smaller polygon on the hydrology map. Also, it is possible to show a bounding box on the hydrology map but will be useful to know how to show an entire shapefile on the map.</p> </li> <li> <p>In the model setup process, while uploading an external dataset, the user must manually enter the description twice. It will be better if the interface can inherit the description itself.</p> </li> </ol>"},{"location":"protocols/survey123_admin/","title":"ArcGIS Survey123 Connect","text":""},{"location":"protocols/survey123_admin/#onboarding-credentialing","title":"Onboarding / Credentialing","text":"<ul> <li>User requests access from Administrator (currently Paul), providing email from User with Name, email, Project / PI</li> <li>Adminstrator adds user to group (existing or new as appropriate)</li> <li>Administrator creates temporary access credentials and emails the user the following information:<ul> <li>Username</li> <li>Tempoary Password</li> <li>Connection url</li> <li>Custom generated url for Trusted Conenction with ESRI cloud and local portal</li> </ul> </li> <li>Auto-emails for forgotten passords currently not working, so Administrator will manually reset.</li> </ul>"},{"location":"protocols/survey123_admin/#launching-application","title":"Launching Application","text":"<ul> <li>TAKE SCREENSHOTS WHEN DOING THIS FOR FIRST TIME</li> <li>desktop app (download and install.  Get links from Paul)</li> <li>requires liscensing from server</li> <li>upper right hamburger = utilities</li> <li>settings --&gt; Add connection (onboarding)</li> <li>onboarding doc will provide url. if valid url, connection will work.</li> <li>select connection, click OK</li> <li>login (sites and stories server is very slow)</li> <li>Icont in top right infers which user / server you're in</li> </ul>"},{"location":"protocols/survey123_admin/#before-you-go","title":"Before you Go","text":""},{"location":"protocols/survey123_admin/#-set-up-a-collection-in-coredb","title":"- Set up a collection in coreDB","text":""},{"location":"protocols/survey123_admin/#creating-new-survey","title":"Creating new survey","text":"<ul> <li>Click 'New Survey' button</li> <li>Give new survey a meaningful name</li> <li>Samples has a lot of precreated sample surveys that feature different features / functions.  Can use as tutorials and delete later</li> <li>Select and create survey</li> <li>Form (preview view), with options on left.<ul> <li>XLSForm opens spreadsheet that is driving the form</li> <li>Update syncs spreadsheet to preview (but doesn't save changes in excel!). (Save in excel also does this.)</li> <li>Files opens to local file system</li> <li>Tools (analyze survey, etc.)</li> <li>Publish, syncs to the portal and creates series of links to share</li> </ul> </li> <li>Details<ul> <li>Description field</li> </ul> </li> <li>Options<ul> <li>Set user permissions, etc.</li> </ul> </li> <li>Map<ul> <li>Set overview and zoom for overall map in survey (?does it do this for multiple?)</li> </ul> </li> <li>Media, basically project folder<ul> <li>CSVs for options</li> </ul> </li> <li>Linked Content<ul> <li>Have to be published at least once for feature to turn on</li> <li>ex. linked maps</li> </ul> </li> <li>Scripts<ul> <li>Custom javascript</li> </ul> </li> <li>Schema<ul> <li>Displays Layers/Tables, fields, field properties </li> </ul> </li> </ul>"},{"location":"protocols/survey123_admin/#modifying-xls-spreadsheet","title":"Modifying XLS spreadsheet","text":"<ul> <li>documentation</li> </ul>"},{"location":"protocols/survey123_admin/#publish","title":"Publish","text":"<ul> <li>Use defaults</li> <li>Analyze survey.  will give info on issues in form (basically a debugger)</li> <li>Resolve issues and publish</li> <li>Check on cloud survey123: put cutom url into new browser window address bar <ul> <li>Repeat login with the same credentials</li> <li>if says 'timed out', click login again</li> </ul> </li> </ul>"},{"location":"protocols/survey123_admin/#review-forms-online","title":"Review Forms online","text":"<ul> <li>Browser windows with tiles for forms, most recent in top left</li> <li>Opens into overview of submissions</li> <li>Collaborate<ul> <li>Survey share links under </li> <li>Controls for who can see, submit, etc.</li> <li>Options to create Shared Update Group for collaborative form development and administration </li> </ul> </li> </ul>"},{"location":"protocols/survey123_admin/#notes","title":"Notes","text":"<ul> <li>forms ONLY live locally until published</li> </ul>"},{"location":"protocols/survey123_user/","title":"ArcGIS Survey123 Connect","text":"<p>Part of the REDI suite of services includes access to the ArcGIS portal and Survey123 data collection tools.  This page will walk you through the steps for getting set up to use Survey 123.</p>"},{"location":"protocols/survey123_user/#onboarding-credentialing","title":"Onboarding / Credentialing","text":"<p>RESI users need credentials in order to utilize the Survey123 tools. To obtain credentials, reach out to the system Administrator [Paul?].  In your email include your Name, email, and the project / PI associated with your request. You will receive an email back with the necessary onboarding information: - Username - Tempoary Password - Connection url - Custom generated url for Trusted Connection with ESRI cloud and local portal - link for downloading the Desktop application</p>"},{"location":"protocols/survey123_user/#forgotten-password","title":"Forgotten password","text":"<p>The automatic password recoery tool is not currently functioning.  If you lose your access credentials / password, please contact the Administrator for a reset.</p>"},{"location":"protocols/survey123_user/#accessing-surveys","title":"Accessing Surveys","text":"<p>To access your organizations surveys, login to the portal url provided, and then navigate to the 'survey 123 app' accessed via the 9 dot grid next to your username in the upper right corner of the screen. </p> <p>The landing screen will show you all the surveys created by you and other members of your organization.  To create a new survey click the '+ New Survey' button.</p> <p>There are 3 options available for creating a new survey: The 'Blank Survey' and 'Template Survey' options use a web interface while the 'Survey 123 Connect' option requires the installation of a Desktop Application.  </p> <p>The web based survey editor provides a simple, intuitive user interface with drag and drop capabilities, however it is limited in some of its features and the ability for more complex operations like nested or conditional questions. Those functionalities are available vie the Survey123 Connect Desktop Application.</p> <p>To use the Web template simply click the 'Get started' button.</p>"},{"location":"protocols/survey123_user/#survey123-connect-advanced-surveys","title":"Survey123 Connect: Advanced Surveys","text":""},{"location":"protocols/survey123_user/#launching-survey123-connect-application","title":"Launching Survey123 Connect Application","text":"<p>Follow the following steps to launch, install and connect to your organization's portal. 1. Download the 64-bit Desktop Application from the Microsoft Store or the [ESRI site] (https://www.esri.com/en-us/arcgis/products/arcgis-survey123/downloads) 2. Install tha application 3. After launching, navigate to the options bar (3 horizontal lines) in the top right and select 'Settings'  4. Click the \"+ Add connection\" button and enter the ArcGIS connection URL provided in your onboarding email 5. Wait while the utility establishes a connection to the server - this may take a few minutes. 6. Sign in using the credentials for the portal you just connected to. </p>"},{"location":"protocols/survey123_user/#advanced-template","title":"Advanced Template","text":"<ul> <li>Click 'New Survey' button</li> <li>Give new survey a meaningful name</li> <li>Samples has a lot of precreated sample surveys that feature different features / functions.  Can use as tutorials and delete later</li> <li>Select and create survey</li> </ul>"},{"location":"protocols/survey123_user/#cd-re-options","title":"cd Re     Options","text":"<ul> <li>Form (preview view), with options on left.<ul> <li>XLSForm opens spreadsheet that is driving the form</li> <li>Update syncs spreadsheet to preview (but doesn't save changes in excel!). (Save in excel also does this.)</li> <li>Files opens to local file system</li> <li>Tools (analyze survey, etc.)</li> <li>Publish, syncs to the portal and creates series of links to share</li> </ul> </li> <li>Details<ul> <li>Description field</li> </ul> </li> <li>Options<ul> <li>Set user permissions, etc.</li> </ul> </li> <li>Map<ul> <li>Set overview and zoom for overall map in survey (?does it do this for multiple?)</li> </ul> </li> <li>Media, basically project folder<ul> <li>CSVs for options</li> </ul> </li> <li>Linked Content<ul> <li>Have to be published at least once for feature to turn on</li> <li>ex. linked maps</li> </ul> </li> <li>Scripts<ul> <li>Custom javascript</li> </ul> </li> <li>Schema<ul> <li>Displays Layers/Tables, fields, field properties </li> </ul> </li> </ul>"},{"location":"protocols/survey123_user/#modifying-xls-spreadsheet","title":"Modifying XLS spreadsheet","text":"<ul> <li>documentation</li> </ul>"},{"location":"protocols/survey123_user/#publish","title":"Publish","text":"<ul> <li>Use defaults</li> <li>Analyze survey.  will give info on issues in form (basically a debugger)</li> <li>Resolve issues and publish</li> <li>Check on cloud survey123: put cutom url into new browser window address bar <ul> <li>Repeat login with the same credentials</li> <li>if says 'timed out', click login again</li> </ul> </li> </ul>"},{"location":"protocols/survey123_user/#review-forms-online","title":"Review Forms online","text":"<ul> <li>Browser windows with tiles for forms, most recent in top left</li> <li>Opens into overview of submissions</li> <li>Collaborate<ul> <li>Survey share links under </li> <li>Controls for who can see, submit, etc.</li> <li>Options to create Shared Update Group for collaborative form development and administration </li> </ul> </li> </ul>"},{"location":"protocols/survey123_user/#notes","title":"Notes","text":"<ul> <li>forms ONLY live locally until published</li> </ul>"},{"location":"tutorials/","title":"Index","text":"<p>Tutorials Index will go here</p> <p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"tutorials/misc/mkdocs-examples/","title":"Mkdocs examples","text":""},{"location":"tutorials/misc/mkdocs-examples/#admonitions","title":"Admonitions","text":""},{"location":"tutorials/misc/mkdocs-examples/#standard","title":"Standard","text":"<p>Custom name text</p> <p>Look in documentation for alternative icon options. </p> <p>Standard type qualifiers (for the icon): note; abstract; info; tip; success; question; warning; failure; danger; bug; example; quote</p>"},{"location":"tutorials/misc/mkdocs-examples/#collapsible","title":"Collapsible","text":"Custom name text <p>This is defined with ??? rather than !!! to start the admonition.  ???+ defaults to open while ??? defaults closed.</p>"},{"location":"tutorials/misc/mkdocs-examples/#annotations","title":"Annotations","text":"<p>Lorem ipsum dolor sit amet, (1) consectetur adipiscing elit.</p> <ol> <li> I'm an annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be expressed in Markdown.</li> </ol>"},{"location":"tutorials/misc/mkdocs-examples/#nested","title":"Nested","text":"<p>Phasellus posuere in sem ut cursus (1)</p> <p>Lorem ipsum dolor sit amet, (2) consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <ol> <li> I'm an annotation!</li> <li> I'm an annotation as well!</li> </ol>"},{"location":"tutorials/misc/mkdocs-examples/#buttons","title":"Buttons","text":"<p>Test Button</p>"},{"location":"tutorials/misc/mkdocs-examples/#code-blocks","title":"Code Blocks","text":"<p>Include Code Blocks with copy feature</p> <pre><code>import pandas as pd\n</code></pre> bubble_sort_example.py<pre><code>def bubble_sort(items): \n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <pre><code>theme:\n  features:\n    - content.code.annotate # (1) (2)\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> <li>This is another</li> </ol>"},{"location":"tutorials/misc/mkdocs-examples/#content-tabs","title":"Content Tabs","text":"TAB 1TAB 2 <p>Add content here</p> <p>tab 2 content here.</p>"},{"location":"tutorials/python/","title":"Index","text":"<p>Add some kind of write up / guidance here</p>"},{"location":"tutorials/python/test/","title":"Test","text":"<p>Test md to see if the pages structure is working</p>"}]}